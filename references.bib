@article{SuykensJohanA.K2001SVMA,
issn = {0947-3580},
abstract = {In recent years neural networks as multilayer perceptrons and radial basis function networks have been frequently used in a wide range of fields, including control theory, signal processing and nonlinear modelling. A promising new methodology is Support Vector Machines (SVM), which has been originally introduced by Vapnik within the area of statistical learning theory and structural risk minimization. SVM approaches to classification, nonlinear function and density estimation lead to convex optimization problems, typically quadratic programming. However, due to their non-parametric nature, the present SVM methods were basically restricted to static problems. We discuss a method of least squares support vector machines (LS-SVM), which has been extended to recurrent models and use in optimal control problems. We explain how robust nonlinear estimation and sparse approximation can be done by means of this kernel based technique. A short overview of hyperparameter tuning methods is given. SVM methods are able to learn and generalize well in large dimensional input spaces and have outperformed many existing methods on benchmark data sets. Its full potential in a dynamical systems and control context remains to be explored.},
journal = {European journal of control},
pages = {311--327},
volume = {7},
publisher = {Elsevier Ltd},
number = {2-3},
year = {2001},
title = {Support Vector Machines: A Nonlinear Modelling and Control Perspective},
copyright = {2001 European Control Association (EUCA)},
language = {eng},
address = {Cachan},
author = {Suykens, Johan A.K},
keywords = {Neural control ; Ridge regression ; Support vector machines ; Kernel based methods ; Robust estimation ; Statistical learning theory ; Regularization ; Feedforward and recurrent neural networks ; Exact sciences and technology ; Control system synthesis ; Applied sciences ; Connectionism. Neural networks ; Control theory. Systems ; Artificial intelligence ; Computer science; control theory; systems},
}

@inproceedings{Brabanter2003LSSVMlabTU,
  title={LS-SVMlab Toolbox User's Guide version 1.7},
  author={K. D. Brabanter and P. Karsmakers and F. Ojeda and C. Alzate and J. D. Brabanter and K. Pelckmans and B. Moor and J. Vandewalle and J. Suykens},
  year={2003}
}

@article{VanGestelTony2002Bffl,
issn = {0899-7667},
abstract = {The Bayesian evidence framework has been successfully applied to the design of multilayer perceptrons (MLPs) in the work of MacKay. Nevertheless, the training of MLPs suffers from drawbacks like the nonconvex optimization problem and the choice of the number of hidden units. In support vector machines (SVMs) for classification, as introduced by Vapnik, a nonlinear decision boundary is obtained by mapping the input vector first in a nonlinear way to a high-dimensional kernel-induced feature space in which a linear large margin classifier is constructed. Practical expressions are formulated in the dual space in terms of the related kernel function, and the solution follows from a (convex) quadratic programming (QP) problem. In least-squares SVMs (LS-SVMs), the SVM problem formulation is modified by introducing a least-squares cost function and equality instead of inequality constraints, and the solution follows from a linear system in the dual space. Implicitly, the least-squares formulation corresponds to a regression formulation and is also related to kernel Fisher discriminant analysis. The least-squares regression formulation has advantages for deriving analytic expressions in a Bayesian evidence framework, in contrast to the classification formulations used, for example, in gaussian processes (GPs). The LS-SVM formulation has clear primal-dual interpretations, and without the bias term, one explicitly constructs a model that yields the same expressions as have been obtained with GPs for regression. In this article, the Bayesian evidence framework is combined with the LS-SVM classifier formulation. Starting from the feature space formulation, analytic expressions are obtained in the dual space on the different levels of Bayesian inference, while posterior class probabilities are obtained by marginalizing over the model parameters. Empirical results obtained on 10 public domain data sets show that the LS-SVM classifier designed within the Bayesian evidence framework consistently yields good generalization performances.},
journal = {Neural computation},
pages = {1115--1148},
volume = {15},
publisher = {M i t press},
number = {5},
year = {2002},
title = {Bayesian framework for least-squares support vector machine classifiers, Gaussian processes, and kernel Fisher discriminant analysis},
language = {eng},
author = {Van Gestel, Tony and Suykens, Johan and Lanckriet, G and Lambrechts, A and De Moor, Bart and Vandewalle, Joos},
keywords = {classification},
}

@article{articletime,
author = {Espinoza, Marcelo and Falck, Tillmann and Suykens, Johan and De Moor, Bart and Leuven, K},
year = {2008},
month = {01},
pages = {},
title = {Time series prediction using ls-svms}
}


@article{SuykensJ.A.K2003Asvm,
issn = {1045-9227},
abstract = {In this paper, we present a simple and straightforward primal-dual support vector machine formulation to the problem of principal component analysis (PCA) in dual variables. By considering a mapping to a high-dimensional feature space and application of the kernel trick (Mercer theorem), kernel PCA is obtained as introduced by Scholkopf et al. (2002). While least squares support vector machine classifiers have a natural link with the kernel Fisher discriminant analysis (minimizing the within class scatter around targets +1 and -1), for PCA analysis one can take the interpretation of a one-class modeling problem with zero target value around which one maximizes the variance. The score variables are interpreted as error variables within the problem formulation. In this way primal-dual constrained optimization problem interpretations to the linear and kernel PCA analysis are obtained in a similar style as for least square-support vector machine classifiers.},
journal = {IEEE transactions on neural networks},
pages = {447--450},
volume = {14},
publisher = {IEEE},
number = {2},
year = {2003},
title = {A support vector machine formulation to PCA analysis and its kernel version},
copyright = {Copyright 2004 Elsevier Science B.V., Amsterdam. All rights reserved.},
language = {eng},
address = {PISCATAWAY},
author = {Suykens, J.A.K and Van Gestel, T and Vandewalle, J and De Moor, B},
keywords = {Support vector machines ; Constraint optimization ; Support vector machine classification ; Scattering ; Predictive models ; Knowledge management ; Kernel ; Analysis of variance ; Least squares methods ; Principal component analysis ; Kernel principal component analysis (PCA) ; Kernel methods ; PCA analysis ; Least squares-support vector machine (LS-SVM) ; SVMs ; Engineering ; Technology ; Computer Science ; Engineering, Electrical & Electronic ; Computer Science, Artificial Intelligence ; Computer Science, Theory & Methods ; Computer Science, Hardware & Architecture ; Science & Technology ; Vector analysis ; Principal components analysis},
}




